{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python39464bitnornickelhackathonvenvd613d22d404a40db9a6ba6813ab9f8f2",
   "display_name": "Python 3.9.4 64-bit ('nornickel_hackathon': venv)",
   "metadata": {
    "interpreter": {
     "hash": "6ff5ba63cadd4cebb77ff5f5920f06abd45cf6fcd599c26827739a1f3b2848dc"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# submit generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "from src.features import generate_features\n",
    "from src.models.model import ModelSick\n",
    "\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Считывание данных\n",
    "\n",
    "sot = pd.read_csv('data/sotrudniki.csv', sep = ';')\n",
    "rod = pd.read_csv('data/rodstvenniki.csv', sep = ';')\n",
    "ogrv = pd.read_csv('data/OGRV.csv', sep = ';')\n",
    "weather = pd.read_csv('data/Weather.csv', sep = '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "1 (99214, 597)\n(99214, 597)\n2 (99214, 608)\n3 (99214, 608)\n"
    }
   ],
   "source": [
    "X, y = generate_features(sot, rod, ogrv, weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = {'feature_fraction': 0.4,\n",
    " 'lambda_l1': 12.9,\n",
    " 'lambda_l2': 14.600000000000001,\n",
    " 'learning_rate': 0.1,\n",
    " 'max_bin': 310.0,\n",
    " 'max_depth': 5.0,\n",
    " 'min_data_in_leaf': 1200.0,\n",
    " 'num_leaves': 84.0,\n",
    " 'path_smooth': 2.325}\n",
    "best_params = {'num_leaves': int(best['num_leaves'])\n",
    "            , 'max_bin': int(best['max_bin'])\n",
    "            , 'max_depth': int(best['max_depth'])\n",
    "            , 'learning_rate': round(best['learning_rate'], 3)\n",
    "            , 'path_smooth': round(best['path_smooth'], 3)\n",
    "            , 'lambda_l1': round(best['lambda_l1'], 3)\n",
    "            , 'lambda_l2': round(best['lambda_l2'], 3)\n",
    "            , 'min_data_in_leaf': int(best['min_data_in_leaf'])\n",
    "            , 'feature_fraction':round(best['feature_fraction'], 3)         \n",
    "            , 'objective': 'binary' \n",
    "            , 'metric': 'auc'\n",
    "            , 'nthread': 7\n",
    "             }\n",
    "nround = 15\n",
    "\n",
    "params = {i: best_params for i in range(1,13)}\n",
    "nrounds = {i: nround for i in range(1,13)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Деление на трейн и тест для последующего прогноза final\n",
    "\n",
    "X_train = X[X.date < pd.to_datetime('2019-08-01')]\n",
    "y_train = y[X.date < pd.to_datetime('2019-08-01')]\n",
    "\n",
    "X_train = X_train[~ y_train.isna().any(axis=1)]\n",
    "y_train = y_train[~ y_train.isna().any(axis=1)]\n",
    "\n",
    "X_test = X[X.date == pd.to_datetime('2019-08-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": " positive: 6805, number of negative: 56016\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.615915 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 100380\n[LightGBM] [Info] Number of data points in the train set: 62821, number of used features: 561\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.108324 -> initscore=-2.107980\n[LightGBM] [Info] Start training from score -2.107980\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 6802, number of negative: 56019\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.374659 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 100380\n[LightGBM] [Info] Number of data points in the train set: 62821, number of used features: 561\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.108276 -> initscore=-2.108474\n[LightGBM] [Info] Start training from score -2.108474\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 6869, number of negative: 55952\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.455999 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 100380\n[LightGBM] [Info] Number of data points in the train set: 62821, number of used features: 561\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.109342 -> initscore=-2.097476\n[LightGBM] [Info] Start training from score -2.097476\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 6941, number of negative: 55880\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.352424 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 100380\n[LightGBM] [Info] Number of data points in the train set: 62821, number of used features: 561\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.110489 -> initscore=-2.085761\n[LightGBM] [Info] Start training from score -2.085761\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 7038, number of negative: 55783\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.338935 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 100380\n[LightGBM] [Info] Number of data points in the train set: 62821, number of used features: 561\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.112033 -> initscore=-2.070145\n[LightGBM] [Info] Start training from score -2.070145\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 7126, number of negative: 55695\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028436 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 100380\n[LightGBM] [Info] Number of data points in the train set: 62821, number of used features: 561\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.113433 -> initscore=-2.056140\n[LightGBM] [Info] Start training from score -2.056140\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 7221, number of negative: 55600\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.339855 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 100380\n[LightGBM] [Info] Number of data points in the train set: 62821, number of used features: 561\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.114946 -> initscore=-2.041190\n[LightGBM] [Info] Start training from score -2.041190\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 7280, number of negative: 55541\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.342031 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 100380\n[LightGBM] [Info] Number of data points in the train set: 62821, number of used features: 561\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.115885 -> initscore=-2.031991\n[LightGBM] [Info] Start training from score -2.031991\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 7285, number of negative: 55536\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050393 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 100380\n[LightGBM] [Info] Number of data points in the train set: 62821, number of used features: 561\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.115964 -> initscore=-2.031214\n[LightGBM] [Info] Start training from score -2.031214\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 7407, number of negative: 55414\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.327416 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 100380\n[LightGBM] [Info] Number of data points in the train set: 62821, number of used features: 561\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.117906 -> initscore=-2.012407\n[LightGBM] [Info] Start training from score -2.012407\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 7519, number of negative: 55302\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.357339 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 100380\n[LightGBM] [Info] Number of data points in the train set: 62821, number of used features: 561\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.119689 -> initscore=-1.995376\n[LightGBM] [Info] Start training from score -1.995376\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 7535, number of negative: 55286\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.377226 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 100380\n[LightGBM] [Info] Number of data points in the train set: 62821, number of used features: 561\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.119944 -> initscore=-1.992961\n[LightGBM] [Info] Start training from score -1.992961\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nf1_score_max = 0.39359267734553777\nf1_score_max = 0.3495145631067961\nf1_score_max = 0.33789148654609613\nf1_score_max = 0.3288888888888889\nf1_score_max = 0.3331968890708146\nf1_score_max = 0.3385178408051235\nf1_score_max = 0.3262969588550984\nf1_score_max = 0.3228819212808539\nf1_score_max = 0.3008849557522124\nf1_score_max = 0.28118113706478626\nf1_score_max = 0.292534281361097\nf1_score_max = 0.32110643415514134\n"
    }
   ],
   "source": [
    "%autoreload 2\n",
    "\n",
    "model = ModelSick(params, nrounds, 4, 3)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Представление результата работы модели в плоский вид\n",
    "\n",
    "one_line_sub = pd.DataFrame(columns = ['hash_tab_num','date','target'])\n",
    "for i in range(1,13):\n",
    "    temp_result = pd.DataFrame(columns = ['hash_tab_num','date','target'])\n",
    "    temp_result['hash_tab_num'] = predictions['hash_tab_num']\n",
    "    temp_result['date'] = pd.to_datetime('2019-09-01') + pd.DateOffset(months=i-1)\n",
    "    temp_result['target'] = predictions['y_' + str(i)]\n",
    "    one_line_sub = pd.concat([one_line_sub, temp_result], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>hash_tab_num</th>\n      <th>date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>2019-09-01</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>2019-10-01</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>2019-11-01</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>2019-12-01</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>2020-01-01</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   hash_tab_num       date\n0             0 2019-09-01\n1             0 2019-10-01\n2             0 2019-11-01\n3             0 2019-12-01\n4             0 2020-01-01"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Подготовка файла submission\n",
    "submission = pd.read_csv('data/submission_check.csv', sep = ';')\n",
    "submission.date = pd.to_datetime(submission.date, format='%Y-%m-%d')\n",
    "submission.drop('target', axis =1, inplace = True)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_line_sub['hash_tab_num'] = pd.to_numeric(one_line_sub['hash_tab_num'])\n",
    "submission_final = pd.merge(submission, one_line_sub, how = 'left', on = ['hash_tab_num','date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_final.to_csv('my_submission_14.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "aiohttp==3.7.4.post0\nappdirs==1.4.4\nappnope==0.1.2\nasync-timeout==3.0.1\nattrs==20.3.0\nbackcall==0.2.0\nchardet==4.0.0\nclikit==0.6.2\ncloudpickle==1.6.0\ncmdstanpy==0.9.68\nconvertdate==2.3.2\ncrashtest==0.3.1\ncycler==0.10.0\nCython==0.29.17\ndecorator==4.4.2\nephem==3.7.7.1\net-xmlfile==1.0.1\nfuture==0.18.2\nhijri-converter==2.1.1\nholidays==0.11.1\nhttpstan==4.4.2\nhyperopt==0.2.5\nidna==3.1\nipykernel==5.5.3\nipython==7.22.0\nipython-genutils==0.2.0\njedi==0.18.0\njoblib==1.0.1\njupyter-client==6.1.12\njupyter-core==4.7.1\nkiwisolver==1.3.1\nkorean-lunar-calendar==0.2.1\nlightgbm==3.2.1\nLunarCalendar==0.0.9\nlz4==3.1.3\nmarshmallow==3.11.1\nmatplotlib==3.4.1\nmultidict==5.1.0\nnetworkx==2.5.1\nnumpy==1.20.2\nopenpyxl==3.0.7\npandas==1.2.4\nparso==0.8.2\npastel==0.2.1\npatsy==0.5.1\npexpect==4.8.0\npickleshare==0.7.5\nPillow==8.2.0\nplotly==4.14.3\npmdarima==1.8.0\nprompt-toolkit==3.0.18\nprophet==1.0.1\nptyprocess==0.7.0\nPygments==2.8.1\npylev==1.3.0\nPyMeeus==0.5.11\npyparsing==2.4.7\npysimdjson==3.2.0\npystan==2.19.1.1\npython-dateutil==2.8.1\npytz==2021.1\npyzmq==22.0.3\nretrying==1.3.3\nscikit-learn==0.24.1\nscipy==1.6.2\nsetuptools-git==1.2\nsix==1.15.0\nsklearn==0.0\nstatsmodels==0.12.2\nthreadpoolctl==2.1.0\ntornado==6.1\ntqdm==4.60.0\ntraitlets==5.0.5\ntransliterate==1.10.2\ntyping-extensions==3.7.4.3\nujson==4.0.2\nurllib3==1.26.4\nwcwidth==0.2.5\nwebargs==7.0.1\nyarl==1.6.3\n"
    }
   ],
   "source": [
    "!pip freeze\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}